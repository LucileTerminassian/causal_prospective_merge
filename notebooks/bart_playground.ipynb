{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pymc_bart as pmb\n",
    "import torch \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coal = np.loadtxt(pm.get_data(\"coal.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize data\n",
    "years = int(coal.max() - coal.min())\n",
    "bins = years // 4\n",
    "hist, x_edges = np.histogram(coal, bins=bins)\n",
    "# compute the location of the centers of the discretized data\n",
    "x_centers = x_edges[:-1] + (x_edges[1] - x_edges[0]) / 2\n",
    "# xdata needs to be 2D for BART\n",
    "x_data = x_centers[:, None]\n",
    "# express data as the rate number of disaster per year\n",
    "y_data = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_host_sample = 80 \n",
    "sigma_error = 1 \n",
    "d = 10 \n",
    "A = torch.randn((d,d))\n",
    "A = 1/(torch.det(A)) * A\n",
    "\n",
    "T_allocation_host = torch.randn(d)\n",
    "T_allocation_host = 100/torch.norm(T_allocation_host)*T_allocation_host\n",
    "\n",
    "mu_nc = torch.randn(d)\n",
    "mu_nc = 1/torch.norm(mu_nc)*mu_nc\n",
    "\n",
    "mu_c = torch.randn(d)\n",
    "mu_c = 1/torch.norm(mu_c)*mu_c\n",
    "\n",
    "mu = torch.concat([mu_nc,mu_c])\n",
    "\n",
    "\n",
    "X_host_no_T = (torch.randn((n_host_sample,d)) @ A ) \n",
    "T_host = torch.bernoulli(torch.sigmoid(X_host_no_T@ T_allocation_host))\n",
    "X_host_times_T = (T_host.unsqueeze(dim=0).T * X_host_no_T)\n",
    "X_host = torch.concat([X_host_no_T,X_host_times_T],dim=1)\n",
    "\n",
    "Y_host = X_host @ mu\n",
    "Y_host = (1/Y_host.norm()) * Y_host + sigma_error * torch.randn_like(Y_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbcausalforest import XBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_host_sample = 80 \n",
    "sigma_error = 1 \n",
    "d = 10 \n",
    "A = torch.randn((d,d))\n",
    "A = 1/(torch.det(A)) * A\n",
    "\n",
    "T_allocation_host = torch.randn(d)\n",
    "T_allocation_host = 100/torch.norm(T_allocation_host)*T_allocation_host\n",
    "\n",
    "mu_nc = torch.randn(d)\n",
    "mu_nc = 1/torch.norm(mu_nc)*mu_nc\n",
    "\n",
    "mu_c = torch.randn(d)\n",
    "mu_c = 1/torch.norm(mu_c)*mu_c\n",
    "\n",
    "mu = torch.concat([mu_nc,mu_c])\n",
    "\n",
    "\n",
    "X_host_no_T = (torch.randn((n_host_sample,d)) @ A ) \n",
    "T_host = torch.bernoulli(torch.sigmoid(X_host_no_T@ T_allocation_host))\n",
    "X_host_times_T = (T_host.unsqueeze(dim=0).T * X_host_no_T)\n",
    "X_host = torch.concat([X_host_no_T,X_host_times_T],dim=1)\n",
    "\n",
    "Y_host = X_host @ mu\n",
    "Y_host = (1/Y_host.norm()) * Y_host + sigma_error * torch.randn_like(Y_host)\n",
    "\n",
    "Y = np.array(Y_host,dtype=np.float32)\n",
    "T = np.array(T_host,dtype=np.int32)\n",
    "X = np.array(X_host_no_T,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES_PR  = 200\n",
    "NUM_TREES_TRT = 100\n",
    "\n",
    "cf = XBCF(\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    num_sweeps=50, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    num_trees_pr=NUM_TREES_PR,\n",
    "    num_trees_trt=NUM_TREES_TRT,\n",
    "    num_cutpoints=100,\n",
    "    Nmin=1,\n",
    "    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    #mtry_trt=X.shape[1], \n",
    "    tau_pr = 0.6 * np.var(Y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    tau_trt = 0.1 * np.var(Y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    beta_pr= 2, # shrinkage (tree depth)\n",
    "    alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = XBCF(\n",
    "    # #model=\"Normal\",\n",
    "    # parallel=True, \n",
    "    num_sweeps=500, \n",
    "    burnin=15,\n",
    "    max_depth=250,\n",
    "    # num_trees_pr=NUM_TREES_PR,\n",
    "    # num_trees_trt=NUM_TREES_TRT,\n",
    "    # num_cutpoints=100,\n",
    "    # Nmin=1,\n",
    "    # #mtry_pr=X1.shape[1], # default 0 seems to be 'all'\n",
    "    # #mtry_trt=X.shape[1], \n",
    "    # tau_pr = 0.6 * np.var(Y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,\n",
    "    # tau_trt = 0.1 * np.var(Y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,\n",
    "    # alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "    # beta_pr= 2, # shrinkage (tree depth)\n",
    "    # alpha_trt= 0.95, # shrinkage for treatment part\n",
    "    # beta_trt= 2,\n",
    "    p_categorical_pr = 0,\n",
    "    p_categorical_trt = 0,\n",
    "    # standardize_target=True, # standardize y and unstandardize for prediction\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XBCF(num_sweeps = 500, burnin = 15, max_depth = 250, Nmin = 1, num_cutpoints = 100, no_split_penality = 4.605170185988092, mtry_pr = 10, mtry_trt = 10, p_categorical_pr = 0, p_categorical_trt = 0, num_trees_pr = 30, alpha_pr = 0.95, beta_pr = 1.25, tau_pr = 0.02693632364273071, kap_pr = 16.0, s_pr = 4.0, pr_scale = False, num_trees_trt = 10, alpha_trt = 0.25, beta_trt = 3.0, tau_trt = 0.013468161821365357, kap_trt = 16.0, s_trt = 4.0, trt_scale = False, verbose = False, parallel = False, set_random_seed = False, random_seed = 0, sample_weights_flag = True, a_scaling = True, b_scaling = True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.fit(\n",
    "    x_t=np.zeros_like(X), # Covariates treatment effect\n",
    "    x=X, # Covariates outcome (including propensity score)\n",
    "    y=Y,  # Outcome\n",
    "    z=T, # Treatment group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 500)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.predict(X,return_mean=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setattrs(_self, **kwargs):\n",
    "    for k,v in kwargs.items():\n",
    "        setattr(_self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattrs(cf, num_sweeps=50, burnin=15,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from below, normal predict returns causal predictions and has a predictive version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.sigma_draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09439312, -0.36632751,  0.09668792, ...,  0.38641616,\n",
       "        -0.11607723, -0.04188406],\n",
       "       [-0.09439312, -0.47813647,  0.09668792, ...,  0.38641616,\n",
       "        -0.11607723, -0.06905785],\n",
       "       [-0.09439312, -0.47813647,  0.09668792, ...,  0.38641616,\n",
       "        -0.11607723,  0.11227725],\n",
       "       ...,\n",
       "       [-0.09439312, -0.36632751,  0.09668792, ...,  0.38641616,\n",
       "        -0.11607723, -0.22321917],\n",
       "       [ 0.01981385, -0.47813647,  0.09668792, ...,  0.38641616,\n",
       "        -0.11607723, -0.22321917],\n",
       "       [ 0.01981385, -0.47813647,  0.09668792, ...,  0.38641616,\n",
       "        -0.11607723, -0.22321917]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.predict(X,return_mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eig_comp_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meig_comp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predictions_in_EIG_obs_form,compute_EIG_obs_from_samples\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eig_comp_utils'"
     ]
    }
   ],
   "source": [
    "from eig_comp_utils import predictions_in_EIG_obs_form,compute_EIG_obs_from_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setattrs(self, **kwargs):\n",
    "    for k,v in kwargs.items():\n",
    "        setattr(self, k, v)\n",
    "\n",
    "class BayesianCausalForest():\n",
    "        \n",
    "        def __init__(self, prior_hyperparameters):\n",
    "            self.sigma_0_sq = prior_hyperparameters['sigma_0_sq']\n",
    "            self.p_categorical_pr = prior_hyperparameters['p_categorical_pr']\n",
    "            self.p_categorical_trt= prior_hyperparameters['p_categorical_trt']\n",
    "            self.model = XBCF(p_categorical_pr = self.p_categorical_pr,p_categorical_trt = self.p_categorical_trt) \n",
    "        \n",
    "        def set_model_atrs(self,**kwargs):\n",
    "                 for k,v in kwargs.items():\n",
    "                    setattr(self.model, k, v)\n",
    "        \n",
    "        def store_train_data(self,X,Y,T):\n",
    "            self.X_train = X\n",
    "            self.Y_train = Y\n",
    "            self.T_train = T\n",
    "        \n",
    "        def posterior_sample_predictions(self, X, n_samples):\n",
    "\n",
    "            \"\"\"\"Returns n samples from the posterior\"\"\"\n",
    "            self.set_model_atrs(num_sweeps = n_samples)\n",
    "            self.model.fit(\n",
    "                    x_t=self.X_train, # Covariates treatment effect\n",
    "                    x=self.X_train, # Covariates outcome (including propensity score)\n",
    "                    y=self.Y_train,  # Outcome\n",
    "                    z=self.T_train, # Treatment group\n",
    "                    )\n",
    "            return self.model.predict(X,return_mean=False)\n",
    "        \n",
    "        def samples_obs_EIG(self,X,n_samples_outer_expectation,n_samples_inner_expectation):\n",
    "                n_samples = n_samples_outer_expectation*(n_samples_inner_expectation+1)\n",
    "                predicitions = self.posterior_sample_predictions(X=X,   n_samples=n_samples  )\n",
    "                predictions_in_form = predictions_in_EIG_obs_form(predicitions, n_samples_outer_expectation, n_samples_inner_expectation)   \n",
    "                return compute_EIG_obs_from_samples(predictions_in_form, self.sigma_0_sq**(1/2))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_hyperparameters = {'sigma_0_sq':1,'p_categorical_pr':0,'p_categorical_trt':0 }\n",
    "bcf = BayesianCausalForest(prior_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf.store_train_data(X=X,Y=Y,T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13422537, -0.14250628, -0.1081355 , ...,  0.07980274,\n",
       "         0.08245827, -0.06705811],\n",
       "       [ 0.13422537, -0.14250628, -0.1081355 , ...,  0.07980274,\n",
       "         0.08245827, -0.09174895],\n",
       "       [ 0.13422537, -0.14250628, -0.1081355 , ...,  0.07980274,\n",
       "         0.08245827, -0.09174895],\n",
       "       ...,\n",
       "       [ 0.06124566, -0.14250628, -0.1081355 , ...,  0.07980274,\n",
       "         0.08245827, -0.18068693],\n",
       "       [ 0.06124566, -0.14250628, -0.1081355 , ...,  0.07980274,\n",
       "         0.08245827, -0.18068693],\n",
       "       [ 0.13422537, -0.14250628, -0.1081355 , ...,  0.07980274,\n",
       "         0.08245827, -0.18068693]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcf.posterior_sample_predictions(X,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
