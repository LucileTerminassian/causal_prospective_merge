{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucile/causal_info_gain/pjake/lib/python3.9/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor) \n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from rct_data_generator import *\n",
    "from outcome_models import *\n",
    "from plotting_functions import *\n",
    "from mcmc_bayes_update import *\n",
    "from eig_comp_utils import *\n",
    "from research_exp_utils import *\n",
    "import uci_dataset as dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "n_host = 200\n",
    "sigma_prior = 1\n",
    "sigma_rand_error = 1\n",
    "\n",
    "power_x, power_x_t = 1, 0\n",
    "std_true_y = 1 # Standard deviation for the true Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight.whole</th>\n",
       "      <th>Weight.shucked</th>\n",
       "      <th>Weight.viscera</th>\n",
       "      <th>Weight.shell</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.4280</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.5190</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.1965</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>0.3995</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Length  Diameter  Height  Weight.whole  Weight.shucked  \\\n",
       "0  1.0   0.655     0.530   0.190        1.4280          0.4930   \n",
       "1  0.0   0.685     0.520   0.165        1.5190          0.6990   \n",
       "2  0.0   0.440     0.365   0.125        0.5160          0.2155   \n",
       "3  1.0   0.465     0.380   0.135        0.5790          0.2080   \n",
       "4  1.0   0.630     0.500   0.180        1.1965          0.5140   \n",
       "\n",
       "   Weight.viscera  Weight.shell  Rings  \n",
       "0          0.3180        0.5650     18  \n",
       "1          0.3685        0.4000     10  \n",
       "2          0.1140        0.1550     10  \n",
       "3          0.1095        0.2200     14  \n",
       "4          0.2325        0.3995      8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone = dataset.load_abalone()\n",
    "abalone['Sex'] = abalone['Sex'].map({'M': 0, 'F': 1})\n",
    "abalone.dropna(inplace=True)\n",
    "resampled_abalone = [abalone.sample(frac=1, replace=True) for _ in range(5*(10**3))]\n",
    "# Concatenate resampled DataFrames\n",
    "abalone = pd.concat(resampled_abalone, ignore_index=True)\n",
    "\n",
    "causal_param_first_index = power_x * np.shape(abalone)[1] + 1\n",
    "\n",
    "abalone.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_x_dim = np.shape(abalone)[1]\n",
    "initial_n_entire_data = np.shape(abalone)[0]\n",
    "x_distributions={}\n",
    "\n",
    "for column in abalone.columns:\n",
    "    x_distributions[column] = abalone[column].values\n",
    "\n",
    "# simulate T\n",
    "T_rct = np.random.randint(2, size=initial_n_entire_data)\n",
    "p_assigned_to_host = lambda X, T, eps: sigmoid(1 + 20*X['Sex'] - X['Weight.viscera'] + 12*np.sqrt(X['Diameter']) + 12*X['Weight.shell'] + 30*T + eps)\n",
    "p_assigned_to_cand2 = lambda X, T, eps: sigmoid(1 + 20*X['Sex'] - X['Weight.viscera'] + 12*np.sqrt(X['Diameter']) + 12* X['Weight.shell'] + 30*T + eps)\n",
    "\n",
    "d = 1 + initial_x_dim*(power_x) + 1 + len(x_distributions)*(power_x_t)\n",
    "outcome_function = lambda X, T, eps: 1 + 1 * X['Sex'] - 1 * X['Weight.viscera'] + np.log(X['Weight.whole']) - X['Height'] + 4 * T + 2* X['Weight.shucked']*T + 24* X['Weight.shell']*T + 0* X['Weight.shucked']*T + eps \n",
    "# outcome_function = lambda X, T, eps: 1 + 1 * X[:,0] - 1 * X[:,1] + 1 * X[:,2] + 4 * T + 2* X[:,0]*T + 24* X[:,1]*T + 0* X[:,2]*T + eps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters for Bayesian update on host\n",
    "prior_mean = torch.zeros(d)\n",
    "beta_0, sigma_0_sq, inv_cov_0 = prior_mean, sigma_rand_error,torch.eye(d)\n",
    "prior_hyperparameters = {'beta_0': beta_0, 'sigma_0_sq': sigma_0_sq,\"inv_cov_0\":inv_cov_0}\n",
    "\n",
    "# Hyperparameters for Bayesian update on host\n",
    "warmup_steps = 50\n",
    "max_tree_depth = 5\n",
    "\n",
    "# Number of samples used to estimate outer expectation\n",
    "n_samples_for_expectation = 50\n",
    "m_samples_for_expectation = 1000\n",
    "\n",
    "\n",
    "# Incorporating sqrt constraint into MCMC samples\n",
    "n_mc = (n_samples_for_expectation * (m_samples_for_expectation+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_both_candidates_list = [200, 500, 1000]\n",
    "proportion = 1 #n_cand2 = prorportion * n_both_candidates_list\n",
    "std_true_y = 1\n",
    "\n",
    "\n",
    "data_parameters = {'n_both_candidates_list': n_both_candidates_list, 'proportion':proportion,  'x_distributions':None,\\\n",
    "                   'p_assigned_to_cand2':p_assigned_to_cand2, 'n_host':n_host, 'power_x':power_x, \\\n",
    "                    'power_x_t':0, 'outcome_function':outcome_function, 'std_true_y':std_true_y, 'causal_param_first_index':causal_param_first_index}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EIG from samples for varying sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_outer_expectation = 100\n",
    "n_samples_inner_expectation = 200\n",
    "n_causal_outer_exp = 100\n",
    "n_causal_inner_exp = 200\n",
    "\n",
    "sampling_parameters = {'n_samples_inner_expectation':n_samples_inner_expectation, 'n_samples_outer_expectation':n_samples_outer_expectation, \\\n",
    "                       'n_causal_inner_exp':n_causal_inner_exp, 'n_causal_outer_exp':n_causal_outer_exp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_additional = 0\n",
    "if not plot_additional:\n",
    "    dict_additional_plots_obs_from_samples = dict_additional_plots_caus_from_samples = {'complementary':0, 'twin': 0, 'twin_treated': 0, 'twin_untreated': 0}\n",
    "\n",
    "else:\n",
    "    exact_data = generate_exact_real_data_varying_sample_size(abalone, T_rct, data_parameters)\n",
    "    dict_additional_plots_obs_from_samples, dict_additional_plots_caus_from_samples = linear_eig_from_samples_exact_datasets(exact_data, data_parameters, prior_hyperparameters, n_mc, sampling_parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_mirror) n=509 != n_mirror (1000)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bart_eig_from_samples_varying_sample_size() missing 2 required positional arguments: 'conditional_model_param' and 'sampling_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (n_seeds):\n\u001b[1;32m      5\u001b[0m     data \u001b[38;5;241m=\u001b[39m generate_data_from_real_varying_sample_size(abalone, T_rct, data_parameters)\n\u001b[0;32m----> 6\u001b[0m     EIG_obs_samples, EIG_caus_samples \u001b[38;5;241m=\u001b[39m \u001b[43mbart_eig_from_samples_varying_sample_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(EIG_obs_samples_across_seeds)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m         EIG_obs_samples_across_seeds\u001b[38;5;241m=\u001b[39m EIG_obs_samples\n",
      "\u001b[0;31mTypeError\u001b[0m: bart_eig_from_samples_varying_sample_size() missing 2 required positional arguments: 'conditional_model_param' and 'sampling_parameters'"
     ]
    }
   ],
   "source": [
    "n_seeds = 2\n",
    "EIG_obs_samples_across_seeds, EIG_caus_samples_across_seeds = [], []\n",
    "\n",
    "for i in range (n_seeds):\n",
    "    data = generate_data_from_real_varying_sample_size(abalone, T_rct, data_parameters)\n",
    "    EIG_obs_samples, EIG_caus_samples = bart_eig_from_samples_varying_sample_size(data, data_parameters, prior_hyperparameters, sampling_parameters)\n",
    "    if len(EIG_obs_samples_across_seeds)==0:\n",
    "        EIG_obs_samples_across_seeds= EIG_obs_samples\n",
    "        EIG_caus_samples_across_seeds = EIG_caus_samples\n",
    "    else:\n",
    "        EIG_obs_samples_across_seeds = np.vstack((EIG_obs_samples_across_seeds, EIG_obs_samples))\n",
    "        EIG_caus_samples_across_seeds = np.vstack((EIG_caus_samples_across_seeds, EIG_caus_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_l1 ='p_assigned_to_host=p_assigned_to_cand2, approx 0.8 treated in host'\n",
    "text_l2 = 'n_samples_inner_expectation = '+str(n_samples_inner_expectation)+ 'n_samples_outer_expectation = '+str(n_samples_outer_expectation)+', n_host = '+str(n_host)+', sigma_prior = sigma_rand_error = '+str(sigma_rand_error)\n",
    "text_l4 = 'n_host = 200, sigma_prior = sigma_rand_error = 1, n_causal_inner_exp = '+str(n_causal_inner_exp) \n",
    "\n",
    "\n",
    "plot_additional = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_obs_samples = '/Users/lucile/causal_info_gain/plots/eig_obs_samples'\n",
    "path_obs_samples = 0\n",
    "alpha = 0.3\n",
    "\n",
    "plot_array(dict_additional_plots_obs_from_samples, n_both_candidates_list, EIG_obs_samples_across_seeds, axis_names= ['Sample size of candidate datasets', 'EIG predictive'], names=['complementary','twin'],\n",
    "           text= text_l1+ '\\n' + text_l2+ '\\n' + text_l3+ '\\n' + text_l4, title= 'EIG predictive', save=path_obs_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_caus_samples = '/Users/lucile/causal_info_gain/plots/eig_caus_samples'\n",
    "path_caus_samples = 0\n",
    "alpha = 0.3\n",
    "\n",
    "plot_array(dict_additional_plots_caus_from_samples, n_both_candidates_list, EIG_caus_samples_across_seeds, axis_names= ['Sample size of candidate datasets', 'EIG causal'], names=['complementary','twin'],\n",
    "           text= text_l1+ '\\n' + text_l2+ '\\n' + text_l3+ '\\n' + text_l4, title= 'EIG causal', save=path_caus_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjake_kernel",
   "language": "python",
   "name": "pjake_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
