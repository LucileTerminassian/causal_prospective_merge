{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucile/causal_info_gain/pjake/lib/python3.9/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor) \n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from rct_data_generator import *\n",
    "from outcome_models import *\n",
    "from plotting_functions import *\n",
    "from mcmc_bayes_update import *\n",
    "from eig_comp_utils import *\n",
    "from research_exp_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eclamp</th>\n",
       "      <th>gestatcat1</th>\n",
       "      <th>gestatcat2</th>\n",
       "      <th>gestatcat3</th>\n",
       "      <th>gestatcat4</th>\n",
       "      <th>gestatcat5</th>\n",
       "      <th>gestatcat6</th>\n",
       "      <th>gestatcat7</th>\n",
       "      <th>gestatcat8</th>\n",
       "      <th>gestatcat9</th>\n",
       "      <th>...</th>\n",
       "      <th>brstate_reg</th>\n",
       "      <th>feduc6</th>\n",
       "      <th>dfageq</th>\n",
       "      <th>nprevistq</th>\n",
       "      <th>data_year</th>\n",
       "      <th>crace</th>\n",
       "      <th>birmon</th>\n",
       "      <th>dtotord_min</th>\n",
       "      <th>dlivord_min</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   eclamp  gestatcat1  gestatcat2  gestatcat3  gestatcat4  gestatcat5  \\\n",
       "0     0.0         0.0         1.0         1.0         0.0         1.0   \n",
       "1     0.0         0.0         1.0         1.0         0.0         0.0   \n",
       "2     0.0         0.0         1.0         0.0         1.0         0.0   \n",
       "3     0.0         0.0         0.0         0.0         1.0         0.0   \n",
       "4     0.0         0.0         1.0         1.0         0.0         0.0   \n",
       "\n",
       "   gestatcat6  gestatcat7  gestatcat8  gestatcat9  ...  brstate_reg  feduc6  \\\n",
       "0         1.0         0.0         0.0         1.0  ...          5.0     2.0   \n",
       "1         0.0         0.0         1.0         0.0  ...          5.0     5.0   \n",
       "2         0.0         0.0         0.0         1.0  ...          5.0     2.0   \n",
       "3         1.0         0.0         0.0         0.0  ...          5.0     4.0   \n",
       "4         0.0         0.0         0.0         0.0  ...          5.0     4.0   \n",
       "\n",
       "   dfageq  nprevistq  data_year  crace  birmon  dtotord_min  dlivord_min    T  \n",
       "0     1.0        0.0        0.0    0.0     0.0          3.0          3.0  1.0  \n",
       "1     8.0        0.0        0.0    0.0     0.0          1.0          1.0  1.0  \n",
       "2     0.0        0.0        0.0    0.0     0.0          1.0          1.0  0.0  \n",
       "3     6.0        0.0        0.0    0.0     0.0          2.0          2.0  1.0  \n",
       "4     7.0        0.0        0.0    1.0     0.0          3.0          3.0  0.0  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../\"\n",
    "data_with_groundtruth, x, t, y = get_data('twins', path)\n",
    "data_with_groundtruth.dropna(inplace=True)\n",
    "data_with_groundtruth = data_with_groundtruth.rename(columns={'t': 'T', 'y': 'Y'})\n",
    "XandT = data_with_groundtruth.drop(columns=['Y','y0','y1','ite'])\n",
    "XandT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_candidate_sites = 10\n",
    "\n",
    "min_sample_size_cand = 150\n",
    "max_sample_size_cand = 300\n",
    "host_sample_size = 400 \n",
    "desired_initial_sample_size = 10**4\n",
    "XandT = XandT.sample(n=desired_initial_sample_size, replace=True, random_state=42)\n",
    "added_T_coef = 50 # to increase importance of T\n",
    "\n",
    "outcome_function = None\n",
    "std_true_y = 1\n",
    "power_x = 1\n",
    "power_x_t = 1\n",
    "sigma_rand_error = 1\n",
    "true_beta_great_0_prop = 0.8\n",
    "\n",
    "exp_parameters = {'number_of_candidate_sites': number_of_candidate_sites+1, 'min_sample_size_cand': min_sample_size_cand, \\\n",
    "                'max_sample_size_cand': max_sample_size_cand, 'host_sample_size': host_sample_size, 'outcome_function': outcome_function, \\\n",
    "                'std_true_y': std_true_y, 'power_x': power_x, 'power_x_t': power_x_t}\n",
    "\n",
    "causal_param_first_index = power_x*np.shape(XandT)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_random_sites_from(data, exp_parameters, added_T_coef=1):\n",
    "    \n",
    "    candidates = {}\n",
    "    sample_size, number_covariates = np.shape(data)[0], np.shape(data)[1]\n",
    "    function_indices = {0: lambda X: np.log(X+1), 1: lambda X: X**3, 2: lambda X: X, 3: lambda X: X**2 }\n",
    "    number_of_candidate_sites = exp_parameters['number_of_candidate_sites']\n",
    "    min_sample_size_cand = exp_parameters['min_sample_size_cand']\n",
    "    max_sample_size_cand = exp_parameters['max_sample_size_cand']\n",
    "    outcome_function = None\n",
    "    std_true_y = exp_parameters['std_true_y']\n",
    "    power_x = exp_parameters['power_x']\n",
    "    power_x_t = exp_parameters['power_x_t']\n",
    "    number_features = number_covariates\n",
    "    created_sites = 0\n",
    "    \n",
    "    while created_sites < number_of_candidate_sites:\n",
    "\n",
    "        np.random.seed(np.random.randint(10000))\n",
    "        \n",
    "        selected_features_for_subsampling = np.random.randint(2, size = number_features) \n",
    "        # binary bool vector representing selection for being an input of the sampling function\n",
    "        random_coefs = [np.random.uniform(-10, 10) for _ in range(number_features)] \n",
    "        random_fct_idx = [np.random.randint(0, len(function_indices.keys())) for _ in range(number_features)] \n",
    "        \n",
    "        def p_assigned_to_site(X, T, eps):\n",
    "            result = 0\n",
    "            for j in range(number_features-1):\n",
    "                result += selected_features_for_subsampling[j] * random_coefs[j] * function_indices[random_fct_idx[j]](X[j])\n",
    "            # here i use added_T_coef * random_coefs to increase importance of T\n",
    "            result +=  added_T_coef * random_coefs[-1] *  function_indices[random_fct_idx[-1]](T) #selected_features_for_subsampling[-1]\n",
    "            return sigmoid(result + eps)\n",
    "        \n",
    "        sample_size = np.random.randint(min_sample_size_cand, max_sample_size_cand + 1)  # Add 1 to include max_sample_size_cand\n",
    "\n",
    "        if created_sites==0:\n",
    "            sample_size = exp_parameters['host_sample_size']\n",
    "        design_data_cand = subsample_one_dataset(XandT, p_assigned_to_site, sample_size, power_x, power_x_t, outcome_function, std_true_y, seed=np.random.randint(10000))\n",
    "        design_data_cand = design_data_cand.dropna()\n",
    "        any_nan = design_data_cand.isna().any().any()\n",
    "        if not design_data_cand.empty and not any_nan: # we're appending\n",
    "            candidates[created_sites] = design_data_cand\n",
    "        else:\n",
    "            number_of_candidate_sites+=1 # not appending\n",
    "        created_sites += 1\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/lucile/causal_info_gain/causal_prospective_merge/rct_data_generator.py:144: RuntimeWarning:overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "#dictionnary of random sites\n",
    "candidate_sites = generating_random_sites_from(XandT, exp_parameters, added_T_coef=50)\n",
    "\n",
    "for i, cand in candidate_sites.items():\n",
    "    candidate_sites[i] = pd.concat([cand, data_with_groundtruth.loc[cand.index, 'Y']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = (np.random.randn(152) > true_beta_great_0_prop)\n",
    "beta = beta - np.mean(beta)\n",
    "\n",
    "for i, cand in candidate_sites.items():\n",
    "    candidate_sites[i][\"Y\"] = candidate_sites[i].drop(columns=[\"Y\"]) @ beta \n",
    "    candidate_sites[i][\"Y\"] += 1 * np.random.randn(len(candidate_sites[i][\"Y\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = candidate_sites[0]\n",
    "candidate_sites = {key: value for key, value in candidate_sites.items() if key != 0}\n",
    "XandT_host, Y_host = torch.from_numpy(host.drop(columns=[\"Y\"]).values), torch.from_numpy(host[\"Y\"].values)\n",
    "\n",
    "# Prior parameters for Bayesian update on host\n",
    "d = np.shape(host)[1]-1\n",
    "prior_mean = torch.zeros(d)\n",
    "sigma_prior = 1\n",
    "beta_0, sigma_0_sq, inv_cov_0 = prior_mean, sigma_rand_error,torch.eye(d)\n",
    "prior_hyperparameters = {'beta_0': beta_0, 'sigma_0_sq': sigma_0_sq,\"inv_cov_0\":inv_cov_0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_outer_expectation_obs = 400\n",
    "n_samples_inner_expectation_obs = 800\n",
    "n_samples_outer_expectation_caus = 400\n",
    "n_samples_inner_expectation_caus = 800\n",
    "\n",
    "sampling_parameters = {'n_samples_inner_expectation_obs':n_samples_inner_expectation_obs, 'n_samples_outer_expectation_obs':n_samples_outer_expectation_obs, \\\n",
    "                       'n_samples_inner_expectation_caus':n_samples_inner_expectation_caus, 'n_samples_outer_expectation_caus':n_samples_outer_expectation_caus}\n",
    "\n",
    "eig_results = {\"EIG_obs_from_samples\": [], 'EIG_caus_from_samples':[], \"EIG_obs_closed_form\":[], \"EIG_caus_closed_form\":[], \"EIG_obs_bart\":[], \"EIG_caus_bart\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % treated in host: 48.25%\n"
     ]
    }
   ],
   "source": [
    "print(f\" % treated in host: {round(100 * host['T'].mean(),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a sample size of 166\n",
      " % treated in candidate: 46.99%\n",
      "For a sample size of 253\n",
      " % treated in candidate: 81.82%\n",
      "For a sample size of 226\n",
      " % treated in candidate: 83.19%\n",
      "For a sample size of 172\n",
      " % treated in candidate: 62.79%\n",
      "For a sample size of 284\n",
      " % treated in candidate: 78.17%\n",
      "For a sample size of 159\n",
      " % treated in candidate: 86.16%\n",
      "For a sample size of 263\n",
      " % treated in candidate: 13.69%\n",
      "For a sample size of 232\n",
      " % treated in candidate: 78.02%\n",
      "For a sample size of 205\n",
      " % treated in candidate: 56.1%\n",
      "For a sample size of 165\n",
      " % treated in candidate: 41.82%\n"
     ]
    }
   ],
   "source": [
    "for _,candidate in candidate_sites.items():\n",
    "    print(f\"For a sample size of {np.shape(candidate)[0]}\")\n",
    "    print(f\" % treated in candidate: {round(100 * candidate['T'].mean(),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, candidate in candidate_sites.items():\n",
    "    X_cand = torch.from_numpy(candidate.drop(columns=[\"Y\"]).values)\n",
    "    bayes_reg = BayesianLinearRegression(prior_hyperparameters)\n",
    "    bayes_reg.set_causal_index(causal_param_first_index)\n",
    "    post_host_parameters = bayes_reg.fit(XandT_host, Y_host)\n",
    "    n_samples = n_samples_outer_expectation_obs * (n_samples_inner_expectation_obs + 1)\n",
    "\n",
    "    eig_results[\"EIG_obs_closed_form\"].append(\n",
    "            bayes_reg.closed_form_obs_EIG(X_cand)\n",
    "            )\n",
    "    eig_results[\"EIG_caus_closed_form\"].append(\n",
    "            bayes_reg.closed_form_causal_EIG(X_cand)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eig_results[\"EIG_obs_from_samples\"]=[]\n",
    "# eig_results[\"EIG_caus_from_samples\"]=[]\n",
    "\n",
    "# for i, candidate in candidate_sites.items():\n",
    "#     print(\"from samples \"+str(i))\n",
    "#     X_cand = torch.from_numpy(candidate.drop(columns=[\"Y\"]).values)\n",
    "#     bayes_reg = BayesianLinearRegression(prior_hyperparameters)\n",
    "#     bayes_reg.set_causal_index(causal_param_first_index)\n",
    "#     post_host_parameters = bayes_reg.fit(XandT_host, Y_host)\n",
    "\n",
    "#     eig_results[\"EIG_obs_from_samples\"].append(\n",
    "#             bayes_reg.samples_obs_EIG(\n",
    "#                 X_cand, n_samples_outer_expectation_obs, n_samples_inner_expectation_obs\n",
    "#             )\n",
    "#         )\n",
    "#     eig_results[\"EIG_caus_from_samples\"].append(\n",
    "#             bayes_reg.samples_causal_EIG(\n",
    "#                 X_cand, n_samples_outer_expectation_obs, n_samples_inner_expectation_obs\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge and compute some CATE error\n",
    "merged_datasets = {}\n",
    "\n",
    "for i, candidate in candidate_sites.items():\n",
    "    merged_datasets[i]= pd.concat([host, candidate], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_diff = {}\n",
    "merged_mse = []\n",
    "XandT_host=host.drop(columns=[\"Y\"])\n",
    "\n",
    "X_zero = XandT_host.copy() # we predict on host with T=0 and T=1\n",
    "X_zero.iloc[:,causal_param_first_index:] = 0\n",
    "\n",
    "X_one = XandT_host.copy()\n",
    "X_one.iloc[:,causal_param_first_index:] = XandT_host.iloc[:,:causal_param_first_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and computing ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "merged_mse = []\n",
    "\n",
    "for i, candidate in merged_datasets.items():\n",
    "\n",
    "    XandT_merged = candidate.drop(columns=[\"Y\"])\n",
    "    Y_merged = candidate['Y']\n",
    "\n",
    "    learner = Ridge(fit_intercept=True)\n",
    "    learner.fit(y=Y_merged, X=XandT_merged) # we fit on merged datasets\n",
    "\n",
    "    true_cate = (X_one - X_zero) @ beta\n",
    "\n",
    "    pred_cate = learner.predict(X_one)-learner.predict(X_zero)\n",
    "\n",
    "    merged_mse.append(mean_squared_error(true_cate, pred_cate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our EIGs with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 7, 1, 8, 2, 0, 3, 9, 5]\n",
      "[7, 4, 8, 6, 1, 2, 3, 0, 9, 5]\n",
      "[2, 7, 9, 4, 8, 1, 3, 0, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "obs_eig_ranking_closed_form = sorted(range(len(eig_results[\"EIG_obs_closed_form\"])), key=lambda i: eig_results[\"EIG_obs_closed_form\"][i], reverse=True)\n",
    "print(obs_eig_ranking_closed_form)\n",
    "\n",
    "caus_eig_ranking_closed_form = sorted(range(len(eig_results[\"EIG_caus_closed_form\"])), key=lambda i: eig_results[\"EIG_caus_closed_form\"][i], reverse=True)\n",
    "print(caus_eig_ranking_closed_form)\n",
    "\n",
    "# obs_eig_ranking_from_samples = sorted(range(len(eig_results[\"EIG_obs_from_samples\"])), key=lambda i: eig_results[\"EIG_obs_from_samples\"][i], reverse=True)\n",
    "# print(obs_eig_ranking_from_samples)\n",
    "\n",
    "# caus_eig_ranking_from_samples = sorted(range(len(eig_results[\"EIG_caus_from_samples\"])), key=lambda i: eig_results[\"EIG_caus_from_samples\"][i], reverse=True)\n",
    "# print(caus_eig_ranking_from_samples)\n",
    "\n",
    "true_cate_ranking = sorted(range(len(merged_mse)), key=lambda i: merged_mse[i], reverse=False) # reverse is False because its error terms\n",
    "print(true_cate_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "k =[1,3,5]\n",
    "top_n = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_at_1': [0.0, 0.0],\n",
       " 'precision_at_3': [0.3333333333333333, 0.3333333333333333],\n",
       " 'precision_at_5': [0.6, 0.6],\n",
       " 'tau': [0.06666666666666667, 0.3333333333333333],\n",
       " 'rho': [0.06666666666666665, 0.34545454545454546]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_with_true_rankings={}\n",
    "\n",
    "for val in k:\n",
    "    correlation_with_true_rankings['precision_at_'+str(val)] = []\n",
    "compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, obs_eig_ranking_closed_form, merged_mse=merged_mse, top_n = top_n, k = k)\n",
    "compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, caus_eig_ranking_closed_form,merged_mse=merged_mse, top_n = top_n, k = k)\n",
    "\n",
    "# compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, obs_eig_ranking_from_samples, top_n = top_n, k = k)\n",
    "# compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, caus_eig_ranking_from_samples, top_n = top_n, k = k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  7  1  5  2  4  8  3  9  6]\n",
      "[5, 7, 2, 8, 3, 9, 4, 1, 10, 6]\n",
      "[2, 0, 1, 3, 4, 5, 6, 7, 8, 9]\n",
      "[5, 2, 4, 7, 1, 3, 8, 0, 9, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/lucile/causal_info_gain/pjake/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning:lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### random ranking\n",
    "random_ranking = np.random.choice(np.arange(1, number_of_candidate_sites+1), size=number_of_candidate_sites, replace=False)\n",
    "\n",
    "\n",
    "### ranking by sample size\n",
    "sample_size_order = sorted(candidate_sites.keys(), key=lambda key: -candidate_sites[key].shape[0])\n",
    "\n",
    "\n",
    "### ranking by similarity of covariate distribution\n",
    "mean_vector_host = XandT_host.iloc[:,:causal_param_first_index].mean()\n",
    "cov_matrix_host = XandT_host.iloc[:,:causal_param_first_index].cov()\n",
    "mvn = multivariate_normal(mean=mean_vector_host, cov=cov_matrix_host, allow_singular=1)\n",
    "# get log likelihood of candidate sites\n",
    "log_likelihood_list=[]\n",
    "for i, candidate in candidate_sites.items():\n",
    "    log_likelihoods=mvn.logpdf(candidate.iloc[:,:causal_param_first_index].values)\n",
    "    log_likelihood_list.append(np.mean(log_likelihoods))\n",
    "\n",
    "similarity_cov_distrib_ranking= sorted(range(len(log_likelihood_list)), key=lambda i: log_likelihood_list[i], reverse=True)\n",
    "\n",
    "### ranking by similarity of propensity scores\n",
    "# we fit a propensity score model at target site and store logloss\n",
    "# for each site: we fit the model further on the cand site and compute log\n",
    "# nd assess the loss. Sites associated with loss values with higher discrepancy from the host should have distinct \n",
    "#treatment allocation scheme, and thus be a better fit. \n",
    "\n",
    "ps_model = LogisticRegression(fit_intercept=True)\n",
    "ps_model.fit(XandT_host.iloc[:,:causal_param_first_index], XandT_host['T'])\n",
    "t_host_pred = ps_model.predict(XandT_host.iloc[:,:causal_param_first_index])\n",
    "mse_host = mean_squared_error(t_host_pred, XandT_host['T'])\n",
    "mse_diff_list = []\n",
    "\n",
    "\n",
    "for i, candidate in candidate_sites.items():\n",
    "    # ps_model_copy= copy.deepcopy(ps_model)\n",
    "    # ps_model_copy.fit(candidate.iloc[:,:causal_param_first_index], candidate['T'])\n",
    "    t_cand_pred = ps_model.predict(candidate.iloc[:,:causal_param_first_index]) # predict on host!\n",
    "    mse_cand = abs(mean_squared_error(t_cand_pred, candidate['T']) - mse_host)\n",
    "    mse_diff_list.append(mse_cand)\n",
    "\n",
    "similarity_pscore_ranking = sorted(range(len(mse_diff_list)), key=lambda i: mse_diff_list[i], reverse=True) \n",
    "# the more diff in pscore the better so reverse=True\n",
    "\n",
    "\n",
    "print(random_ranking)\n",
    "print(sample_size_order)\n",
    "print(similarity_cov_distrib_ranking)\n",
    "print(similarity_pscore_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sample_size_order).index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_at_1': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'precision_at_3': [0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.6666666666666666,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333],\n",
       " 'precision_at_5': [0.6, 0.6, 0.4, 0.6, 0.4, 0.6],\n",
       " 'tau': [0.06666666666666667,\n",
       "  0.3333333333333333,\n",
       "  -0.1111111111111111,\n",
       "  0.15555555555555553,\n",
       "  -0.022222222222222223,\n",
       "  0.28888888888888886],\n",
       " 'rho': [0.06666666666666665,\n",
       "  0.34545454545454546,\n",
       "  -0.12727272727272726,\n",
       "  0.1515151515151515,\n",
       "  -0.05454545454545454,\n",
       "  0.32121212121212117]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, list(random_ranking),merged_mse=merged_mse, top_n = top_n, k = k)\n",
    "compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, list(sample_size_order),merged_mse=merged_mse, top_n = top_n, k = k)\n",
    "compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, list(similarity_cov_distrib_ranking),merged_mse=merged_mse, top_n = top_n, k = k)\n",
    "compare_to_ground_truth(correlation_with_true_rankings, true_cate_ranking, list(similarity_pscore_ranking),merged_mse=merged_mse, top_n = top_n, k = k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_at_1</th>\n",
       "      <th>precision_at_3</th>\n",
       "      <th>precision_at_5</th>\n",
       "      <th>tau</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>obs_closed_form</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caus_closed_form</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.345455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>-0.127273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample size</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.151515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similarity_cov_distrib_ranking</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.022222</td>\n",
       "      <td>-0.054545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similarity_pscore_ranking size</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.321212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                precision_at_1  precision_at_3  \\\n",
       "obs_closed_form                            0.0        0.333333   \n",
       "caus_closed_form                           0.0        0.333333   \n",
       "random                                     0.0        0.333333   \n",
       "sample size                                0.0        0.666667   \n",
       "similarity_cov_distrib_ranking             1.0        0.333333   \n",
       "similarity_pscore_ranking size             0.0        0.333333   \n",
       "\n",
       "                                precision_at_5       tau       rho  \n",
       "obs_closed_form                            0.6  0.066667  0.066667  \n",
       "caus_closed_form                           0.6  0.333333  0.345455  \n",
       "random                                     0.4 -0.111111 -0.127273  \n",
       "sample size                                0.6  0.155556  0.151515  \n",
       "similarity_cov_distrib_ranking             0.4 -0.022222 -0.054545  \n",
       "similarity_pscore_ranking size             0.6  0.288889  0.321212  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_with_true_rankings= pd.DataFrame.from_dict(correlation_with_true_rankings)\n",
    "correlation_with_true_rankings.index = ['obs_closed_form', 'caus_closed_form', 'random', 'sample size', 'similarity_cov_distrib_ranking', 'similarity_pscore_ranking size'] #, 'obs_from_samples', 'caus_from_samples']\n",
    "correlation_with_true_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=-0.4222222222222222, pvalue=0.10831349206349207)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendalltau(true_cate_ranking, merged_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bart stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_host, T_host, Y_host = host.drop(columns=['T','Y']).values, host['T'].values.astype(np.int32), host['Y'].values\n",
    "\n",
    "# prior_hyperparameters = {'sigma_0_sq':1, 'p_categorical_pr':0, 'p_categorical_trt':0 }\n",
    "# predictive_model_parameters={\"num_trees_pr\":200,\"num_trees_trt\":100}\n",
    "# conditional_model_param={\"num_trees_pr\":200}\n",
    "\n",
    "\n",
    "# for i, candidate in candidate_sites.items():\n",
    "\n",
    "#     print(\"from samples \"+str(i))\n",
    "#     X_cand, T_cand = candidate.drop(columns=['Y','T']).values, candidate['T'].values.astype(np.int32)\n",
    "\n",
    "#     bcf = BayesianCausalForest(\n",
    "#         prior_hyperparameters,\n",
    "#         predictive_model_parameters=predictive_model_parameters,\n",
    "#         conditional_model_param=conditional_model_param)\n",
    "#     bcf.store_train_data(X=X_host, T=T_host, Y=Y_host)\n",
    "    \n",
    "#     joint_eig = bcf.joint_EIG_calc(X_cand, T_cand, sampling_parameters)\n",
    "\n",
    "#     results[\"EIG_obs_bart\"].append(joint_eig[\"Obs EIG\"])\n",
    "#     results[\"EIG_caus_bart\"].append(joint_eig[\"Causal EIG\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjake_kernel",
   "language": "python",
   "name": "pjake_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
