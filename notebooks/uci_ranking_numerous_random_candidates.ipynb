{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucile/causal_info_gain/pjake/lib/python3.9/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor) \n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from rct_data_generator import *\n",
    "from outcome_models import *\n",
    "from plotting_functions import *\n",
    "from mcmc_bayes_update import *\n",
    "from eig_comp_utils import *\n",
    "from research_exp_utils import *\n",
    "import uci_dataset as dataset\n",
    "\n",
    "from causalml.inference.tree import CausalForestDML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = dataset.load_abalone()\n",
    "initial_data['Sex'] = initial_data['Sex'].map({'M': 0, 'F': 1})\n",
    "initial_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_candidate_sites = 10\n",
    "min_sample_size_cand = 150\n",
    "max_sample_size_cand = 250\n",
    "host_sample_size = 200\n",
    "desired_initial_sample_size = 10**6\n",
    "initial_data = initial_data.sample(n=desired_initial_sample_size, replace=True, random_state=42)\n",
    "\n",
    "\n",
    "outcome_function = lambda X, T, eps: 1 + 1 * X['Sex'] - 1 * X['Weight.viscera'] + np.log(X['Weight.whole']) - \\\n",
    "    X['Height'] + 4 * T + 2* X['Weight.shucked']*T + 24* X['Weight.shell']*T + 0* X['Weight.shucked']*T + eps \n",
    "std_true_y = 1\n",
    "power_x = 1\n",
    "power_x_t = 1\n",
    "sigma_rand_error = 1\n",
    "\n",
    "exp_parameters = {'number_of_candidate_sites': number_of_candidate_sites, 'min_sample_size_cand': min_sample_size_cand, \\\n",
    "                'max_sample_size_cand': max_sample_size_cand, 'host_sample_size': host_sample_size, 'outcome_function': outcome_function, \\\n",
    "                'std_true_y': std_true_y, 'power_x': power_x, 'power_x_t': power_x_t}\n",
    "\n",
    "causal_param_first_index = causal_param_first_index = power_x * np.shape(initial_data)[1] + 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_random_sites_from(data, exp_parameters):\n",
    "    sites = {}\n",
    "    sample_size, number_covariates = np.shape(data)[0], np.shape(data)[1]\n",
    "    function_indices = {0: lambda X: np.log(X+1), 1: lambda X: X**3, 2: lambda X: X, 3: lambda X: X**2}\n",
    "    T = np.random.randint(2, size=sample_size)\n",
    "    number_of_candidate_sites = exp_parameters['number_of_candidate_sites']\n",
    "    min_sample_size_cand = exp_parameters['min_sample_size_cand']\n",
    "    max_sample_size_cand = exp_parameters['max_sample_size_cand']\n",
    "    outcome_function = exp_parameters['outcome_function']\n",
    "    std_true_y = exp_parameters['std_true_y']\n",
    "    power_x = exp_parameters['power_x']\n",
    "    power_x_t = exp_parameters['power_x_t']\n",
    "    number_features = number_covariates + 1\n",
    "    created_sites = 0\n",
    "    \n",
    "    while created_sites < number_of_candidate_sites:\n",
    "        \n",
    "        selected_features_for_subsampling = np.random.randint(2, size = number_features) \n",
    "        # binary bool vector representing selection for being an input of the sampling function\n",
    "        random_coefs = [np.random.uniform(-10, 10) for _ in range(number_features)] \n",
    "        random_fct_idx = [np.random.randint(0, 4) for _ in range(number_features)] \n",
    "        \n",
    "        def p_assigned_to_site(X, T, eps):\n",
    "            result = 0\n",
    "            for j in range(number_features-1):\n",
    "                result += selected_features_for_subsampling[j] * random_coefs[j] * function_indices[random_fct_idx[j]](X[j])\n",
    "            result += selected_features_for_subsampling[-1] * random_coefs[-1] *  function_indices[random_fct_idx[-1]](T)\n",
    "            return sigmoid(result + eps)\n",
    "        \n",
    "        sample_size = np.random.randint(min_sample_size_cand, max_sample_size_cand + 1)  # Add 1 to include max_sample_size_cand\n",
    "        design_data_cand = subsample_one_dataset(data, T, p_assigned_to_site, sample_size, power_x, power_x_t, outcome_function, std_true_y)\n",
    "        if not design_data_cand.empty:\n",
    "            sites[created_sites] = design_data_cand\n",
    "            created_sites += 1\n",
    "\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucile/causal_info_gain/causal_prospective_merge/rct_data_generator.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size of subsampled dataset is0 which is not equal to the wanted sample size of 184)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucile/causal_info_gain/causal_prospective_merge/rct_data_generator.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size of subsampled dataset is0 which is not equal to the wanted sample size of 250)\n"
     ]
    }
   ],
   "source": [
    "#dictionnary of random sites\n",
    "candidate_sites = generating_random_sites_from(initial_data, exp_parameters)\n",
    "host = candidate_sites.popitem()[1]\n",
    "\n",
    "# Prior parameters for Bayesian update on host\n",
    "d = np.shape(host)[1] -1\n",
    "prior_mean = torch.zeros(d)\n",
    "beta_0, sigma_0_sq, inv_cov_0 = prior_mean, sigma_rand_error,torch.eye(d)\n",
    "prior_hyperparameters = {'beta_0': beta_0, 'sigma_0_sq': sigma_0_sq,\"inv_cov_0\":inv_cov_0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_host, Y_host = torch.from_numpy(host.drop(columns=[\"Y\"]).values), torch.from_numpy(host[\"Y\"].values)\n",
    "n_samples_outer_expectation = 20\n",
    "n_samples_inner_expectation = 30\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"EIG_obs_from_samples\": [], 'EIG_caus_from_samples':[], \"EIG_obs_closed_form\":[], \"EIG_caus_closed_form\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,candidate in candidate_sites.items():\n",
    "    X_cand = torch.from_numpy(candidate.drop(columns=[\"Y\"]).values)\n",
    "    bayes_reg = BayesianLinearRegression(prior_hyperparameters)\n",
    "    bayes_reg.set_causal_index(causal_param_first_index)\n",
    "    post_host_parameters = bayes_reg.fit(X_host, Y_host)\n",
    "    n_samples = n_samples_outer_expectation * (n_samples_inner_expectation + 1)\n",
    "\n",
    "    results[\"EIG_obs_from_samples\"].append(\n",
    "            bayes_reg.samples_obs_EIG(\n",
    "                X_cand, n_samples_outer_expectation, n_samples_inner_expectation\n",
    "            )\n",
    "        )\n",
    "    results[\"EIG_caus_from_samples\"].append(\n",
    "            bayes_reg.samples_causal_EIG(\n",
    "                X_cand, n_samples_outer_expectation, n_samples_inner_expectation\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, candidate in candidate_sites.items():\n",
    "    X_cand = torch.from_numpy(candidate.drop(columns=[\"Y\"]).values)\n",
    "    bayes_reg = BayesianLinearRegression(prior_hyperparameters)\n",
    "    bayes_reg.set_causal_index(causal_param_first_index)\n",
    "    post_host_parameters = bayes_reg.fit(X_host, Y_host)\n",
    "    n_samples = n_samples_outer_expectation * (n_samples_inner_expectation + 1)\n",
    "\n",
    "    results[\"EIG_obs_closed_form\"].append(\n",
    "            bayes_reg.closed_form_obs_EIG(X_cand)\n",
    "            )\n",
    "    results[\"EIG_caus_closed_form\"].append(\n",
    "            bayes_reg.closed_form_causal_EIG(X_cand)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge and compute some CATE error\n",
    "merged_datasets = {}\n",
    "\n",
    "for i, candidate in candidate_sites.items():\n",
    "    merged_datasets[i]= pd.concat([host, candidate], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATE_errors = {}\n",
    "merged_datasets = {}\n",
    "\n",
    "from econml.metalearners import TLearner\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "model_y = GradientBoostingRegressor()\n",
    "model_t = GradientBoostingClassifier()\n",
    "\n",
    "for i, candidate in merged_datasets.items():\n",
    "    merged_datasets[i]= pd.concat([host, candidate], axis=0)\n",
    "    X_merged = merged_datasets[i].filter(regex='^(?!T)').copy()\n",
    "    X_merged = X_merged.drop(columns=[\"Y\"])\n",
    "    T_merged = merged_datasets[i]['T']\n",
    "    Y_merged = merged_datasets[i]['Y']\n",
    "\n",
    "    learner = TLearner(model_y=model_y, model_t=model_t)\n",
    "    learner.fit(Y=Y_merged, T=T_merged, X=X_merged)\n",
    "    cate = learner.effect(X_merged)\n",
    "    ### need dataset with ground truth to compute some kind of errors here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjake_kernel",
   "language": "python",
   "name": "pjake_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
